{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codice scalabile per calcolare il sentiment dei commenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si implementa un sistema di **logging** per tracciare l'esecuzione del codice e facilitare il debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazioni\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per caricare i dati\n",
    "def load_data(file_path):\n",
    "    logging.info(f'Caricamento dati da {file_path}')\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvare i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Funzione per salvare i risultati \n",
    "def save_results(df, file_path):\n",
    "    # Verifica se il file esiste\n",
    "    if not os.path.isfile(file_path):\n",
    "        df.to_csv(file_path, index=False, mode='w')\n",
    "    else:\n",
    "        df.to_csv(file_path, index=False, mode='a', header=False)\n",
    "    logging.info(f'Salvati i risultati parziali in {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esiste un tokenizzatore già addestrato su dati testuali quali tweet o commenti social. Può essere utilizzato anche per il italiano, anche se è stato originariamente progettato per testi in stile tweet in inglese. Tuttavia, poiché i tweet possono contenere caratteristiche multilingue simili (ad esempio emoticon, hashtag, abbreviazioni), TweetTokenizer può funzionare ragionevolmente bene per la tokenizzazione di testi in altre lingue, inclusa l'italiano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 22:22:26,483 - WARNING - From c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.5.0/it_core_news_sm-3.5.0-py3-none-any.whl (13.0 MB)\n",
      "     ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/13.0 MB 3.7 MB/s eta 0:00:04\n",
      "      --------------------------------------- 0.2/13.0 MB 2.8 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.4/13.0 MB 2.5 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.5/13.0 MB 2.7 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.6/13.0 MB 2.7 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.8/13.0 MB 3.0 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.0/13.0 MB 3.0 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/13.0 MB 2.9 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.3/13.0 MB 3.0 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.5/13.0 MB 3.1 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/13.0 MB 3.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.7/13.0 MB 3.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.9/13.0 MB 3.1 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.0/13.0 MB 3.1 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/13.0 MB 3.0 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.2/13.0 MB 2.9 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.2/13.0 MB 2.8 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.3/13.0 MB 2.7 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.4/13.0 MB 2.7 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.5/13.0 MB 2.6 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.6/13.0 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.7/13.0 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.8/13.0 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.8/13.0 MB 2.5 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 2.9/13.0 MB 2.5 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.0/13.0 MB 2.5 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/13.0 MB 2.5 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.2/13.0 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.3/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.4/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.5/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.6/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.8/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.0/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.0/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.1/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.2/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.4/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.5/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.6/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.7/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.8/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.0/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.1/13.0 MB 2.4 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.3/13.0 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.6/13.0 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.7/13.0 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.9/13.0 MB 2.6 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.1/13.0 MB 2.6 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.5/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.6/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.8/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.9/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 7.1/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.4/13.0 MB 2.7 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.8/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.1/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.3/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.7/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.8/13.0 MB 2.8 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.3/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.4/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.6/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.7/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.9/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 10.0/13.0 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/13.0 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.4/13.0 MB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.6/13.0 MB 2.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/13.0 MB 2.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 11.0/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.3/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.7/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 12.0/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.3/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/13.0 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/13.0 MB 3.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.6/13.0 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/13.0 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  13.0/13.0 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  13.0/13.0 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 13.0/13.0 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from it-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.23.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\franc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 22:22:30.991868: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING:tensorflow:From c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\") # Carichiamo il modello italiano di spaCy\n",
    "\n",
    "# Funzione per lemmatizzare una lista di token\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))  # Converto i token in una stringa e processo con spaCy\n",
    "    return [token.lemma_ for token in doc]  # Estraggo i lemmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si utilizza una libreria specifica per la tokenizzazione multilingue come spaCy, \n",
    "# che ha modelli pre-addestrati per molte lingue, tra cui l'italiano.\n",
    "\n",
    "tokening = TweetTokenizer() # Tokenizzazione per i tweet\n",
    "stop = stopwords.words('italian')   # Carichiamo le stopwords italiane     \n",
    "punctuation = string.punctuation    # Carichiamo la punteggiatura\n",
    "\n",
    "# Tengo le parole non e più perché potrebbero essere importanti per il sentimento\n",
    "stop = [item for item in stop if item not in [\"non\", \"più\"]]\n",
    "\n",
    "# Funzione per eseguire il preprocessing dei dati\n",
    "def preprocess_data(df):\n",
    "    \n",
    "    # Quando abbiamo a che fare con il testo dei social media, di solito vogliamo identificare URL, \n",
    "    # hashtag e emoticon come oggetti separati e non tokenizzarli in singoli caratteri:\n",
    "    comments_tokenized = df.apply(TweetTokenizer().tokenize)\n",
    "\n",
    "    # Rimuoviamo le stopword\n",
    "    comments_tokenized_stop = comments_tokenized.apply(lambda x: [item for item in x if item not in stop])\n",
    "    \n",
    "    # Rimuoviamo la punteggiatura\n",
    "    comments_tokenized_stop_punct = comments_tokenized_stop.apply(lambda x: [item for item in x if item not in punctuation])\n",
    "\n",
    "    # Lemmatizzazione\n",
    "    comments_tokenized_new_stem = comments_tokenized_stop_punct.apply(lemmatize_tokens)\n",
    "\n",
    "    # Unisco i token lemmatizzati per formare le frasi preprocessate\n",
    "    preprocessed_comments = comments_tokenized_new_stem.apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return preprocessed_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'**NRC Emotion Lexicon** ha annotazioni influenti per le parole inglesi. Nonostante alcune differenze culturali, è stato dimostrato che la maggior parte delle norme affettive sono stabili attraverso le lingue. Pertanto, forniamo versioni del lessico in oltre 100 lingue traducendo i termini inglesi utilizzando Google Translate (agosto 2022).\n",
    "\n",
    "Il lessico è quindi disponibile per l'inglese ma anche per numerose altre lingue, tra cui l'italiano!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Emotion_Lexicon = \"../resources/NRC-Emotion-Lexicon.xlsx\"\n",
    "lexicon_df = pd.read_excel(Emotion_Lexicon, engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {}\n",
    "for word, pos, neg, ant, ang, dis, fea, joy, sad, sur, tru in zip(lexicon_df[\"Italian Translation (Google Translate)\"], lexicon_df[\"Positive\"], lexicon_df[\"Negative\"], lexicon_df[\"Anticipation\"], lexicon_df[\"Anger\"], lexicon_df[\"Disgust\"], lexicon_df[\"Fear\"], lexicon_df[\"Joy\"], lexicon_df[\"Sadness\"], lexicon_df[\"Surprise\"], lexicon_df[\"Trust\"]): \n",
    "    value = []\n",
    "    if pos:\n",
    "        value.append(\"positive\")\n",
    "    if neg:\n",
    "        value.append(\"negative\")\n",
    "    if ant:\n",
    "        value.append(\"anticipation\")\n",
    "    if ang:\n",
    "        value.append(\"anger\")\n",
    "    if dis:\n",
    "        value.append(\"disgust\")\n",
    "    if fea:\n",
    "        value.append(\"fear\")\n",
    "    if joy:\n",
    "        value.append(\"joy\")\n",
    "    if sad:\n",
    "        value.append(\"sadness\")\n",
    "    if sur:\n",
    "        value.append(\"surprise\")\n",
    "    if tru:\n",
    "        value.append(\"trust\")\n",
    "    lexicon[str(word).lower()] = value #lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che calcola il sentiment con NRC-Emotion-Lexicon\n",
    "def nrc_score(df):\n",
    "    # Inizializzo i contatori per i sentimenti\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    # Inizializzo un dizionario per contare le emozioni\n",
    "    emotion_count = {\n",
    "        \"anticipation\": 0,\n",
    "        \"anger\": 0,\n",
    "        \"disgust\": 0,\n",
    "        \"fear\": 0,\n",
    "        \"joy\": 0,\n",
    "        \"sadness\": 0,\n",
    "        \"surprise\": 0,\n",
    "        \"trust\": 0\n",
    "    }\n",
    "    \n",
    "    # Analizzo ogni parola nella frase\n",
    "    for word in df.split():  # La funzione split divide la stringa dagli spazi\n",
    "        word = word.lower()\n",
    "        if word in lexicon:\n",
    "            values = lexicon[word]\n",
    "            # Conto i sentimenti\n",
    "            if \"positive\" in values:\n",
    "                positive_count += 1\n",
    "            if \"negative\" in values:\n",
    "                negative_count += 1\n",
    "            # Conta le emozioni\n",
    "            for emotion in emotion_count.keys():\n",
    "                if emotion in values:\n",
    "                    emotion_count[emotion] += 1\n",
    "\n",
    "    # Determino il sentimento prevalente\n",
    "    if positive_count > negative_count:\n",
    "        predominant_sentiment = \"positive\"\n",
    "    elif negative_count > positive_count:\n",
    "        predominant_sentiment = \"negative\"\n",
    "    else:\n",
    "        predominant_sentiment = \"neutral\"  \n",
    "\n",
    "    # Determina l'emozione prevalente\n",
    "    if all(value == 0 for value in emotion_count.values()):\n",
    "        predominant_emotion = \"none\"  # Nessuna emozione prevalente\n",
    "    else:\n",
    "        predominant_emotion = max(emotion_count, key=emotion_count.get)\n",
    "    \n",
    "    # Restituisco una lista con i due valori prevalenti\n",
    "    return [predominant_sentiment, predominant_emotion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italian BERT Sentiment model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/neuraly/bert-base-italian-cased-sentiment\n",
    "\n",
    "Questo modello esegue l'analisi del sentiment sulle frasi italiane. È stato addestrato a partire da un'istanza di bert-base-italian-cased, e messo a punto su un dataset italiano di tweets, raggiungendo su quest'ultimo l'82% di precisione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def bert_score(df):\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"neuraly/bert-base-italian-cased-sentiment\")\n",
    "\n",
    "    # Load the model, use .cuda() to load it on the GPU\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained(\"neuraly/bert-base-italian-cased-sentiment\")\n",
    "    bert_model.eval()  # Imposta il modello in modalità valutazione\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        sentence = row.comment_body\n",
    "\n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Create tensor, use .cuda() to transfer the tensor to GPU if available\n",
    "        tensor = torch.tensor(input_ids).long().unsqueeze(0)\n",
    "        if torch.cuda.is_available():\n",
    "            tensor = tensor.cuda()\n",
    "            bert_model.cuda()\n",
    "\n",
    "        # Call the model and get the logits\n",
    "        with torch.no_grad():\n",
    "            logits = bert_model(tensor)\n",
    "        \n",
    "        # Remove the fake batch dimension\n",
    "        logits = logits.logits.squeeze(0)\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        proba = nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "        # Unpack the tensor to obtain negative, neutral and positive probabilities\n",
    "        negative, neutral, positive = proba\n",
    "\n",
    "        # Assign probabilities to DataFrame\n",
    "        df.loc[index, 'negative'] = proba[0].item()\n",
    "        df.loc[index, 'neutral'] = proba[1].item()\n",
    "        df.loc[index, 'positive'] = proba[2].item()\n",
    "\n",
    "    # Determine the sentiment label based on maximum probability\n",
    "    df['BERT'] = df[['negative', 'positive', 'neutral']].idxmax(axis=1)\n",
    "\n",
    "    db = df[['comment_id', 'post_id', 'debate_group', 'comment_user_name', 'comment_body', 'comment_score', 'preprocessed_comment', 'negative', 'neutral', 'positive', 'BERT']]\n",
    "\n",
    "    # Salvo i risultati in un file\n",
    "    save_results(db, '../results/BERT_results.csv')\n",
    "\n",
    "    # Rimuovo le colonne di probabilità\n",
    "    df.drop(['negative', 'neutral', 'positive'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEEL-IT: Emotion and Sentiment Classification for the Italian Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/MilaNLProc/feel-it-italian-sentiment\n",
    "\n",
    "Utilizzo un altro classificatore di sentiment, dotato di un corpus di riferimento di post Twitter italiani annotati con quattro emozioni fondamentali: rabbia, paura, gioia, tristezza. Comprimendoli, possiamo anche fare l’analisi del sentiment. \n",
    "\n",
    "PAPER: https://aclanthology.org/2021.wassa-1.8.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from feel_it import SentimentClassifier\n",
    "from feel_it import EmotionClassifier\n",
    "emotion_classifier = EmotionClassifier()\n",
    "sentiment_classifier = SentimentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feel_score(df):\n",
    "    feel_it_scores = []\n",
    "    for sentence in df['preprocessed_comment']:\n",
    "        feel_it_scores.append(sentiment_classifier.predict([sentence])[0])\n",
    "    df['FEEL_IT'] = feel_it_scores\n",
    "    db = df[['comment_id', 'post_id', 'debate_group', 'comment_user_name', 'comment_body', 'comment_score', 'preprocessed_comment', 'FEEL_IT']]\n",
    "\n",
    "    # Salvo i risultati in un file CSV\n",
    "    save_results(db, \"../results/FEEL_IT_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_score(df):\n",
    "    emotion_scores = []\n",
    "    for sentence in df['preprocessed_comment']:\n",
    "        emotion_scores.append(emotion_classifier.predict([sentence])[0])\n",
    "    df['EMOTION'] = emotion_scores\n",
    "    db = df[['comment_id', 'post_id', 'debate_group', 'comment_user_name', 'comment_body', 'comment_score', 'preprocessed_comment', 'EMOTION']]\n",
    "    save_results(db, \"../results/EMOTION_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTILINGUAL: bert-base-multilingual-uncased-sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tratta di un modello bert-base-multilingue-uncased ottimizzato per l'analisi del sentiment sulle recensioni dei prodotti in sei lingue: inglese, olandese, tedesco, francese, spagnolo e **italiano**. Prevede il sentiment della recensione sotto forma di numero di stelle (tra 1 e 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def multilingual_score(df):\n",
    "    # Crea colonne per memorizzare le probabilità\n",
    "    df['1'] = 0\n",
    "    df['2'] = 0\n",
    "    df['3'] = 0\n",
    "    df['4'] = 0\n",
    "    df['5'] = 0\n",
    "\n",
    "     # Itera attraverso ogni riga del DataFrame\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        sentence = row.preprocessed_comment\n",
    "            \n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True, truncation=True, max_length=512)\n",
    "            \n",
    "        # Create tensor, use .cuda() to transfer the tensor to GPU\n",
    "        tensor = torch.tensor(input_ids).long()\n",
    "        # Fake batch dimension\n",
    "        tensor = tensor.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "        # Call the model and get the logits\n",
    "        logits = model(tensor)\n",
    "        # Remove the fake batch dimension\n",
    "        logits = logits.logits.squeeze(0)\n",
    "\n",
    "        # The model was trained with a Log Likelyhood + Softmax combined loss, hence to extract probabilities we need a softmax on top of the logits tensor\n",
    "        proba = nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "        # Aggiungi le probabilità alle colonne appropriate\n",
    "        proba = proba.to(\"cpu\")\n",
    "        df.loc[index, '1'] = proba[0].item()\n",
    "        df.loc[index, '2'] = proba[1].item()\n",
    "        df.loc[index, '3'] = proba[2].item()\n",
    "        df.loc[index, '4'] = proba[3].item()\n",
    "        df.loc[index, '5'] = proba[4].item()\n",
    "\n",
    "    # Calcola il sentimento prevalente\n",
    "    df['MULTILINGUAL'] = df[['1', '2', '3', '4', '5']].idxmax(axis=1)\n",
    "    db = df[['comment_id', 'post_id', 'debate_group', 'comment_user_name', 'comment_body', 'comment_score', 'preprocessed_comment', '1', '2', '3', '4', '5', 'MULTILINGUAL']]\n",
    "    df.drop(columns=['1', '2', '3', '4', '5'], inplace=True)\n",
    "\n",
    "    # Tentare di convertire i valori della colonna 'MULTILINGUAL' in numerici, ignorando gli errori\n",
    "    df['sentiment_pred_numeric'] = pd.to_numeric(df['MULTILINGUAL'], errors='coerce')\n",
    "\n",
    "    # Applicare le trasformazioni solo ai valori numerici\n",
    "    df.loc[df['sentiment_pred_numeric'] >= 4, 'MULTILINGUAL'] = 'positive'\n",
    "    df.loc[df['sentiment_pred_numeric'] <= 2, 'MULTILINGUAL'] = 'negative'\n",
    "    df.loc[df['sentiment_pred_numeric'] == 3, 'MULTILINGUAL'] = 'neutral'\n",
    "\n",
    "    # Rimuovere la colonna temporanea utilizzata per la conversione\n",
    "    df.drop(columns=['sentiment_pred_numeric'], inplace=True)\n",
    "    \n",
    "    save_results(db, \"../results/MULTILINGUAL_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predirre sentiment ed emotion con i modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per predire il sentiment usando un modello specifico\n",
    "def predict(model_name, df):\n",
    "    # Implementa la logica di predizione per ciascun modello\n",
    "    if(model_name == 'NRC'):\n",
    "        tqdm.pandas()  # Per visualizzare la progress bar con tqdm\n",
    "        df['NRC'] = df[\"preprocessed_comment\"].apply(nrc_score)\n",
    "        save_results(df, '../results/NRC_results.csv')\n",
    "    elif(model_name == 'BERT'):\n",
    "        tqdm.pandas()  # Per visualizzare la progress bar con tqdm\n",
    "        df = bert_score(df)\n",
    "    elif(model_name == 'FEEL'):\n",
    "        tqdm.pandas()  # Per visualizzare la progress bar con tqdm\n",
    "        df = feel_score(df)\n",
    "    elif(model_name == 'MULTILINGUAL'):\n",
    "        tqdm.pandas()  # Per visualizzare la progress bar con tqdm\n",
    "        df = multilingual_score(df)\n",
    "    elif(model_name == 'EMOTION'):\n",
    "        tqdm.pandas()  # Per visualizzare la progress bar con tqdm        \n",
    "        df = emotion_score(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etichettatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede etichettando il dataset risultante per maggioranza: si comparano i risultati dei tre modelli più utilizzati e, il sentimento che prevale, sarà quello associato al dato commento. Quando ho tre sentimenti differenti, imposto \"neutral\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def majority_vote(row):\n",
    "    # Ottieni le predizioni per la riga corrente\n",
    "    predictions = [row['BERT'], row['FEEL_IT'], row['MULTILINGUAL']]\n",
    "    \n",
    "    # Conta le occorrenze di ogni predizione\n",
    "    vote_counts = Counter(predictions)\n",
    "    \n",
    "    # Trova la predizione con il maggior numero di voti\n",
    "    majority_vote = vote_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return majority_vote\n",
    "\n",
    "def apply_majority_vote(df):\n",
    "    # Applica la funzione majority_vote a ogni riga del DataFrame\n",
    "    df['majority_vote'] = df.apply(majority_vote, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo di sentiment ed emotion di una discussione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_and_emotion_discussion(df):\n",
    "    # Raggruppare per emozione e calcolare il punteggio totale ponderato per ogni emozione\n",
    "    sentiment_scores = df.groupby('majority_vote')['comment_score'].sum()\n",
    "    emotion_scores = df.groupby('EMOTION')['comment_score'].sum()\n",
    "\n",
    "\n",
    "    # Determinare l'emozione prevalente per ogni gruppo di discussione\n",
    "    prevalent_sentiment = df.groupby('debate_group').apply(lambda group: sentiment_scores.idxmax())\n",
    "    prevalent_emotion = df.groupby('debate_group').apply(lambda group: emotion_scores.idxmax())\n",
    "\n",
    "\n",
    " # Creare il dataframe discussion con le informazioni desiderate\n",
    "    discussion = pd.DataFrame({\n",
    "        'post_id': df['post_id'].iloc[0],\n",
    "        'debate_group': prevalent_emotion.index,\n",
    "        'sentiment': prevalent_sentiment.values,\n",
    "        'emotion': prevalent_emotion.values\n",
    "    })\n",
    "\n",
    "    # Salvare i risultati in un file CSV\n",
    "    save_results(discussion, '../results/discussion_sentiment_and_emotion.csv')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo pesato del sentiment di un post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def post_sentiment(df, relevance):\n",
    "    # Mappa le etichette 'positive', 'negative', 'neutral' a 1, -1, 0 rispettivamente\n",
    "    sentiment_mapping = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "    df['sentiment_numeric'] = df['sentiment'].map(sentiment_mapping)\n",
    "    \n",
    "    # Verifica che il numero di righe nel DataFrame df corrisponda alla lunghezza di relevance\n",
    "    if len(df) != len(relevance):\n",
    "        raise ValueError(\"La lunghezza di relevance deve corrispondere al numero di righe nel DataFrame df.\")\n",
    "    \n",
    "    # Converti relevance in un array NumPy per la moltiplicazione\n",
    "    relevance_array = np.array(relevance)\n",
    "    \n",
    "    # Calcola l'importanza pesata del sentiment numerico per ciascuna discussione\n",
    "    weighted_sentiments = df['sentiment_numeric'] * relevance_array\n",
    "    \n",
    "    # Calcola il sentiment totale del post sommando tutte le importanze pesate\n",
    "    post_sentiment = weighted_sentiments.sum()\n",
    "    \n",
    "    return post_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento della configurazione\n",
    "with open(\"../resources/config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 22:23:14,477 - INFO - Caricamento dati da ../debate_dataset.csv\n"
     ]
    }
   ],
   "source": [
    " # Caricamento dei dati (Si utilizza il dataset generato al termine dell'analisi dei grafi, \n",
    " # composto esclusivamente dai commenti che hanno contribuito attivamente ai dibattiti di ciascun post. \n",
    " # Lo si vuole analizzare per capire se tali commenti abbiano un sentiment positivo o negativo.)\n",
    "df = load_data(config['data']['input_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['197vo6o' '19aeo2k' '1b6cg4q' '1cwqkqe' '1bulhj9' '1d5h5h6' '17z2hci'\n",
      " '17lese9' '10v8sey']\n",
      "Numero di post unici:  9\n"
     ]
    }
   ],
   "source": [
    "# Creo una lista di id_post unici\n",
    "id_posts = df['post_id'].unique()\n",
    "\n",
    "print(id_posts)\n",
    "print(\"Numero di post unici: \", len(id_posts))\n",
    "\n",
    "\n",
    "# Creo una lista di id_post unici\n",
    "debate_groups = df['debate_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro il dataset in modo tale da avere solo i commenti relativi ad un post\n",
    "\n",
    "# Itero sugli id dei post\n",
    "for id_post in id_posts:\n",
    "    # Filtro il dataframe per l'id del post corrente\n",
    "    df_filtered_post = df[df['post_id'] == id_post]\n",
    "    for debate_group in df_filtered_post['debate_group'].unique():\n",
    "        # Filtro il dataframe per il gruppo di dibattito corrente\n",
    "        df_filtered_group = df_filtered_post[df_filtered_post['debate_group'] == debate_group]\n",
    "        \n",
    "        # Stampo il numero di commenti per il post corrente e gruppo di dibattito\n",
    "        print(f\"Numero di commenti per il post {id_post} che hanno partecipato al debate group {debate_group} : {len(df_filtered_group)}\")\n",
    "        \n",
    "        # Isolo le uniche colonne che mi interessano\n",
    "        df_fil = df_filtered_group[['comment_id', 'comment_body']].copy()\n",
    "        \n",
    "        # Estraggo solo i commenti\n",
    "        comments = df_fil['comment_body']\n",
    "        \n",
    "        # Preprocesso i commenti\n",
    "        preprocessed_comments = preprocess_data(comments)\n",
    "        \n",
    "        df_filtered_group[\"preprocessed_comment\"] = preprocessed_comments\n",
    "        \n",
    "        # Utilizzo un indice booleano per assegnare direttamente i valori nel DataFrame originale\n",
    "        df.loc[df_filtered_group.index, 'preprocessed_comment'] = preprocessed_comments\n",
    "        \n",
    "        # Salvo i risultati parziali\n",
    "        save_results(df.loc[df_filtered_group.index, ['comment_id', 'comment_body', 'preprocessed_comment']], \"../results/preprocessed_comments.csv\")\n",
    "\n",
    "        \n",
    "        # Predizioni con i modelli\n",
    "        models = config['models']\n",
    "        for mod in models:\n",
    "            predict(mod, df_filtered_group)\n",
    "        \n",
    "        # Salvo i risultati parziali\n",
    "        save_results(df_filtered_group, \"../results/predictions.csv\")\n",
    "\n",
    "        # Etichetto il sentimento prevalente\n",
    "        apply_majority_vote(df_filtered_group)\n",
    "\n",
    "        # Salvo i risultati parziali\n",
    "        save_results(df_filtered_group, \"../results/majority_vote.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto si analizzano i risultati ottenuti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 00:04:49,993 - INFO - Caricamento dati da ../results/majority_vote.csv\n"
     ]
    }
   ],
   "source": [
    "# Voglio analizzare il sentiment di ciascuna discussione e salvarlo in un file\n",
    "df = load_data(\"../results/majority_vote.csv\")\n",
    "\n",
    "# Filtro il dataset dei risultati ottenuto prima per avere solo le colonne che mi interessano\n",
    "df = df[['comment_id', 'post_id', 'debate_group', 'comment_body', 'comment_score', 'EMOTION', 'majority_vote']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_groups = df['debate_group'].unique()\n",
    "\n",
    "for debate_group in debate_groups:\n",
    "    # Filtro il dataframe per il gruppo di dibattito corrente\n",
    "    df_filtered_group = df[df['debate_group'] == debate_group]\n",
    "    \n",
    "    # Applico la funzione sentiment_discussione per ottenere il sentiment prevalente per ciascuna discussione\n",
    "    sentiment_and_emotion_discussion(df_filtered_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voglio conoscere il sentiment generale del post, ma lo voglio pesato sulla base dell'importanza di ciascuna discussione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 00:04:56,032 - INFO - Caricamento dati da ../results/discussion_sentiment_and_emotion.csv\n"
     ]
    }
   ],
   "source": [
    "sentiment_post = load_data(\"../results/discussion_sentiment_and_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "id_posts = sentiment_post['post_id'].unique()\n",
    "\n",
    "\n",
    "sentiment = []\n",
    "\n",
    "for id_post in id_posts:\n",
    "    # Filtro il dataframe per il gruppo di dibattito corrente\n",
    "    df_filtered_post = sentiment_post[sentiment_post['post_id'] == id_post]\n",
    "    # per ora randomica\n",
    "    relevance = np.random.rand(sentiment_post.shape[0])\n",
    "    # Applico la funzione sentiment_discussione per ottenere il sentiment prevalente per ciascuna discussione\n",
    "    sentiment.append(post_sentiment(sentiment_post, relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-13.102443197410024,\n",
       " -10.748610443179693,\n",
       " -9.780502262993078,\n",
       " -12.963043719922785,\n",
       " -11.204830894620825,\n",
       " -12.619593501385078,\n",
       " -11.37667778844152,\n",
       " -13.159845578664278,\n",
       " -8.391018789333012]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
